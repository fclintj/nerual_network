\title { Deep Neural Networks  }
\author{ Clint Ferrin          }
\date  { Mon Oct 23, 2017      }
\def\class { Neural Networks: ECE 5930 }
\documentclass{article}\makeatletter

\newcommand{\main} {
   % document setup
   \pageSetup
   \createTitlePage
   \includeHeader
   \createTableOfContents

   % content
   \summary
   \program
   \twoclass
   \tenclass
   \appendix
}

%  ┌────────────────────────┐
%  │     Extra Packages     │
%  └────────────────────────┘
    % \usepackage{fontspec}   % allows Unicode in XeLaTex
    % \setmainfont{FreeSerif} % introduction to fonts below 
    % https://tex.stackexchange.com/questions/320096/xelatex-fontspec-cannot-find-fonts 
    % \usepackage{ucharclasses} % allows Unicode in XeLaTex
    \usepackage[utf8]{inputenc}	% allows new character options
    \usepackage[a4paper]{geometry}   % Paper dimensions and margins
    \usepackage{fancyhdr}   % include document header
    \usepackage{amsmath}    % allows equations to be split
    \usepackage{bm}         % use of bold characters in math mode
    \usepackage{enumitem}   % create lists
    \usepackage{graphicx}	% manage images and graphics
    \usepackage{hyperref}	% creates hyper-link color options
    \usepackage{cleveref}	% (\Cref) include "Figure" on \reff 
    \usepackage{xparse}     % include high performing functions 
    \usepackage{xstring}    % StrSubstitute replace character
    \usepackage{floatrow}	% allows placement of figures [H]
    \usepackage{url}    	% package for url links
    \usepackage{titletoc}   % change Table of Contents settings
    \usepackage{caption}    % removes figure from LoF: \caption[]{}
    \usepackage{listings, lstautogobble} % includes ability to input code
    \usepackage{color}      % include colors for 
    \usepackage{courier}    % courier font for listings
    \usepackage{etoolbox}
    \usepackage{tabulary}	% columns size of their contents (on page)
    \usepackage{booktabs}   % allows for \toprule in tables
    \usepackage{subcaption} % add two plots together
    \usepackage[nomessages]{fp} % calculations in latex

    \definecolor{mygreen}{RGB}{28,172,0}	% custom defined colors
    \definecolor{mylilas}{RGB}{170,55,241}
    \definecolor{mymauve}{rgb}{0.58,0,0.82}
    \definecolor{light-gray}{gray}{0.95} %the shade of grey that stack exchange uses

    \lstset {
        language=Python,
        backgroundcolor = \color{light-gray},
        breaklines		= true,
        keywordstyle    = \color{blue},
        morekeywords    = [2]{1}, keywordstyle=[2]{\color{black}},
        identifierstyle = \color{black},
        stringstyle     = \color{mylilas},
        commentstyle    = \color{mygreen},
        numbers         = left,
        numberstyle     = {\tiny \color{black}},	% size of the numbers
        numbersep       = 6pt, 						% distance of numbers from text
        emph            = [1]{as, for, end, break}, % bold for, end, break...
        emphstyle 		= [1]\color{red}, 			% emphasis
        basicstyle		= \footnotesize\ttfamily,	% set font to courier
        frameround      = ffff,                     % TR, BR, BL, TL. t(round)|f(flat)
        frame           = single,                   % single line all around
        showstringspaces= false,                    % blank spaces appear as written
        autogobble      = true
    }

%  ┌────────────────────────┐
%  │      Common Tasks      │
%  └────────────────────────┘
    \DeclareDocumentCommand{\commontasks}{m} {
        \href{https://drive.google.com/open?id=0B5NW7S3txe5UTE0xSHJHNWxJbEE}{\underline{this link}} 
        \texttt{PYTHON}
        \lstinputlisting[language=Python]{../python/3_linear.py}

        % two figures side by side
        \begin{figure}[H]
            \includegraphics[width=.45\linewidth]{./media/1-nearest.pdf}\hfill 
            \includegraphics[width=.45\linewidth]{./media/5-nearest.pdf}
            \caption[k=1 and k=5 Nearest Neighbor]{Left: k=1 Nearest Neighbor. Right: k=5 Nearest Neighbor}
            \label{fig:kNearOneAndFive}
        \end{figure}

    }

%  ┌────────────────────────┐
%  │   General Functions    │
%  └────────────────────────┘
    % function to create magnitude bars around a function
    \newcommand{\norm}[1]{\left\lVert#1\right\rVert}

    \DeclareDocumentCommand{\newFigureTwo}{m m m m m m m m} {
        % \FPeval{opposite}{1-#9}
        \begin{figure}[H]
        \centering
        \begin{subfigure}{.5\textwidth}
          \centering
          \includegraphics[width=#3\linewidth]{#1}
          \caption{#2}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
          \centering
          \includegraphics[width=#6\linewidth]{#4}
          \caption{#5}
        \end{subfigure}
        \caption{#7}
        \label{#8}
        \end{figure}
    }

    \DeclareDocumentCommand{\reff}{m} {
        \edef\link{#1}
        \hspace{-0.5em}\hyperref[\link]{\Cref*{\link}} \hspace{-0.65em}
    }

    \DeclareDocumentCommand{\newFigure}{m m m m} {
        \edef\path{#1} \edef\figcaption{#2} \edef\size{#3}  

        % add size if not present
        \IfNoValueTF{#3} { % if 2 and 3 are NULL
            \def\size{0.75}
            }{}

            % add caption if not present
        \IfNoValueTF{#2} { % if 2 and 3 are NULL
            %\expandafter\StrSubstitute\expandafter{\temp}{-}{ }[\output]
            \newcommand\helphere{\temp}
            \StrBehind{\helphere}{/}[\figcaption]
        }{}

        \begin{figure}[H]
            \begin{center}
                \includegraphics[width=\size\textwidth]{\path}
                % I deleted the capitalize function because it wouldn't pass []
                % \capitalisewords{}
                \caption{\figcaption}
                \label{#4} % label gets rid of type and -.
            \end{center}
        \end{figure} 
    }

%  ┌────────────────────────┐
%  │   Content Functions    │
%  └────────────────────────┘
    \newcommand{\pageSetup} {

        \let\Title\@title
        \let\Date\@date
        \let\Author\@author

        % \patchcmd{\subsection}{\bfseries}{\normalsize}{}{}
        % \patchcmd{\subsection}{0.5em}{-0.5em}{}{}
        % \renewcommand{\thesubsection}{\normalsize\hspace{-1em}}

        % makes subsection appear in-line
        \renewcommand\subsection{\@startsection{subsubsection}{4}{\z@}%
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {-1.5ex \@plus -.2ex}% Formerly 1.5ex \@plus .2ex
                                     {\normalfont\normalsize\bfseries}}        \renewcommand{\thesubsection}{\hspace{-1em}}

                                     % turns off section numbers
                                     % \renewcommand{\thesection}{\hspace{-1em}}
                                     % \renewcommand{\partname}{}
                                     % \renewcommand{\thepart}{}

        \newgeometry{left=1in,bottom=1in,right=1in,top=1in} % page dims
        \setlength\parindent{0pt}	% set no tab on new paragraphs
        \parskip = \baselineskip	% set single skip after paragraphs
        \setlist{nosep,after=\vspace{\baselineskip}} % remove space on list
        }\hypersetup{				% hyper-links environment
            colorlinks,
            linkcolor	= {black},
            citecolor	= {black},
            urlcolor	= {black},
            pdfborderstyle={/S/U/W 1}
        }

    \newcommand{\createTitlePage} {
        \vspace*{2.5cm}
        \begin{center}
            \thispagestyle{empty}
            

            \huge{\Title} 

            % line
            \vspace{0.25em}
            \line(1,0){250}\normalsize 

            \vspace{5mm}
            \class 

            \vspace{1cm}
                \begin{center}
                \includegraphics[width=0.85\textwidth]{media/title-page.pdf}\par
                    Figure: Two Hidden Layer Neural Network 
                \end{center}
            \vspace{2.5cm}

            \Author \vspace{-1em}

            Utah State University \vspace{-1em}

            \Date           \vspace{-1em}

            \pagenumbering{gobble} 
            \newpage
        \end{center}
    }

    \newcommand{\createTableOfContents} {
        \pagenumbering{roman}
        \clearpage
        % \newdimen\punktik
        % \def\cvak{\ifdim\punktik<6pt \global\punktik=3pt \else\global\punktik=3pt \fi}
        % \def\tocdots{\cvak\leaders\hbox to10pt{\kern\punktik.\hss}\hfill}
        % \titlecontents{section}[0em]{\vskip -1em}{}{\itshape}{\hfill\thecontentspage}
        % \titlecontents{subsection}[1em]{\vskip -1em}{}{} {\tocdots\thecontentspage}

        
        \tableofcontents 

        \clearpage
        \renewcommand*\listfigurename{\normalsize{List of Figures}}
        \listoffigures

        \renewcommand*\listtablename{\normalsize{List of Tables}}
        \listoftables

        \newpage
        \pagenumbering{arabic}
    }

    \newcommand{\includeHeader} {
        \pagestyle{fancy}
        \fancyhf{}
        % \fancyhead[L]{Top Left}
        \fancyhead[L]{\Title}
        \fancyhead[R]{\nouppercase\leftmark}
        % \fancyhead[R]{Top Right}
        \renewcommand{\headrulewidth}{0.5pt}
        %\fancyfoot[L]{Bottom Left}
        \fancyfoot[C]{\thepage}
        %\fancyfoot[R]{Bottom Right}
        \renewcommand{\footrulewidth}{0.5pt}
    }

%  ┌────────────────────────┐
%  │    Written Content     │
%  └────────────────────────┘
    \DeclareDocumentCommand{\summary}{} {
        \section{Summary}\label{sec:summary}
        Neural Networks have applications in image recognition, data compression, and even stock market prediction. The basic concept behind Neural Networks is simply depicted as seen on the main figure of title page. This paper presents the basic structure for machine learning on classified data using a randomly generated data-set (2-classes), and the MNIST data-set (10 classes).

        The MNIST data-set consists of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is 28 × 28 so that when it is vectorized it has dimension 784. The MNIST data-set is ideal for machine learning because of the variable nature of handwriting and the limited classes of numbers.

        Ten numbers from the data-set can been seen in \reff{fig:mnist}.

        \newFigure{./media/mnist.png}{10 Digits from the MNIST Data-set}{.6}{fig:mnist}
        The remainder of the paper will be dedicated to analyzing the effectiveness of Neural Networks in identifying correctly different classes of data such as the numbers seen in \reff{fig:mnist}.
    } 

    \DeclareDocumentCommand{\program}{} {
        \section{Program Description}\label{sec:program}
        
        The generic neural network that I wrote can have any number of layers, neurons, and activation function passed to it for an \texttt{n} demensional input with \texttt{k} number of classes. The Network is initialized by specifying  \texttt{num\_inputs}, \texttt{num\_outputs}, \texttt{batch\_size}, and \texttt{epics}.
        
        In testing the different classes in this paper, the ReLU (Rectified Linear Unit) was used for the majority of the classification problems. 

        The desired data is read in, and any activation functions are defined for the different layers. As seen in the heading \texttt{\# input layer}, the layers are created by passing the number of inputs that they will receive, the number of desired neurons, and the type of activation function. The final layer seen in the code snippet below does not have an activation function because by default softmax is run on the output of the network. In this way, the output option can easily be changed between the softmax and sigmoid functions.
        
        Additional parameters exist for the initialization of the network, but they are optional parameters. Such variables include the momentum $\beta$ and step size $\eta$, as seen in the initialization of the Neural Network.

        \lstinputlisting[language=Python,firstline=12,lastline=39,]{../python/deepnn.py}

        For a full description of the different classes, such as the class \texttt{layer} and \texttt{activation\_function}, see \reff{app:code-listing}. 

        The Training of the system is done by back propagation, which will be discusses in later detail later on.
    }

    \DeclareDocumentCommand{\twoclass}{} {
        \section{Two-class Classifier}\label{sec:two-class}
        The data set from \texttt{classasgntrain1.dat} is a grouping of data centered around 10 different points with a Gaussian Distribution for each class. I split the data into 80\% training data and 20\% testing data using the function seen in the listing below:

        \lstinputlisting[language=Python,firstline=274,lastline=302,]{../python/deepnn.py}

        The network trained the data using sigmoid functions, and it produced the output seen in \reff{fig:two-class-bound}:
        
        \newFigureTwo
        {./media/two-class-bound.pdf}{80\% Training Data, 20\% Testing Data}{.9}
        {./media/two-class-bound-mse.pdf}{Mean Squared Error for 80\%, 20\% Data}{.9}
        {Trained Output for 80 Training Data, 20 Testing Data with 0.8 Momentum}{fig:two-class-bound}
        
        The resulting listing from the program showed that it correctly classified the small batch of test data with only 3 mistakes with a 92.5\% accuracy. 

        \lstinputlisting[language={}]{./media/two-class-boud-output.txt}

        In previous processing, I found that the classification methods in \reff{tab:compare} performed with the following errors in percent. Note that the Bayes Optimal Classifier performed the best because it knew the true distribution of the data. 

        \begin{table}[H]
            \begin{tabulary}{\linewidth}{rccc}	
                && \multicolumn{2}{c}                   {Errors in \%}      \\
                \toprule % --------------------------------------------------
                        Method	            & Run-time &    Training  &  Test     \\ 
                \toprule % ---------------------------------------------------
                Linear Regression 		        &  1.23s   &      14.5    &  20.49    \\
                Quadratic Regression	        &  1.70s   &      14.5    &  20.44    \\
                Linear Discriminant Analysis    &  2.49s   &      15.0    &  19.98    \\
                Quadratic Discriminant Analysis	&  3.26s   &      14.5    &  20.23    \\
                Logistic Regression     	    &  2.00s   &      14.0    &  20.00    \\
                1-Nearest Neighbor	    	    &  35.02s  &      00.0    &  21.83    \\
                5-Nearest Neighbor	    	    &  37.92s  &      12.0    &  20.29    \\
                15-Nearest Neighbor             &  36.47s  &      16.0    &  19.25    \\
                Bayes Naive 	                &  1.22s   &      14.0    &  20.04    \\
                Bayes Optimal Classifier        &  0.20s   &      14.0    &  19.14    \\
                \bottomrule % ------------------------------------------------
            \end{tabulary}\caption{Binary Classifier Performance Comparison}\label{tab:compare}  \end{table} 

        To compare the Neural Network with the other classifiers, I trained on all data points from the \texttt{classasgntrain1.dat} data set and ran it on 20000 randomly generated data points. The results can be seen in \reff{fig:two-class-bound-20000}

        \newFigureTwo
        {./media/two-class-20000.pdf}{Training on All 200 data points}{.9}
        {./media/two-class-20000-mse.pdf}{MSE for the Training Routine}{.9}
        {Trained Output for 200 Data Points in \texttt{classasgntrain1.dat} with 0.8 Momentum and Step Size of 0.1}{fig:two-class-bound-20000}
        
        \lstinputlisting[language={}]{./media/two-class-20000.txt}

        The Neural Network returned an error of 19.44\%, which puts its results just behind the Bayes Optimal Classifier and the k-nearest neighbor approach. It is also important to note that the momentum term has a significant effect on the speed at which the Mean Squared Error drops. \reff{fig:two-class-comp-momentum} shows the dramatic speed difference that the momentum has on the convergence of the Mean Squared Error. Increasing the momentum to 0.8 did not have a significant effect on the percent of errors. 

        \newFigureTwo
        {./media/two-class-20000-mse-no-momentum.pdf}{MSE with No Momentum}{.9}
        {./media/two-class-20000-mse.pdf}{MSE with 0.8 Momentum}{.9}
        {Comparison of Two-class Classifier with and without Momentum}{fig:two-class-comp-momentum}
    }
    
    \DeclareDocumentCommand{\tenclass}{} {
        \section{Ten-class Classifier}\label{sec:ten-class}
        
        % Now train your neural network using the training data from the MNIST database, using 300 neurons in the hidden layer. Obviously, for the 10 digits, you’ll need to have ten neurons in the output layer, and you should use the softmax function in the output layer.  Use a batch of size 100. Use at least three different values of the step size paremeter. For each of these values of step size parameter, make a plot of the MSE as a function of iteration every 50 iterations. (You may need to experiment to determine how many iterations you need.)

        % TODO: Test your neural network using the test data from the MNIST database. Report the probability of successful classification.

        % Comment on how many iterations are needed, and how long (in clock time) it takes to train.  + Create a network with two hidden layers, where the first hidden layer has 5 neurons and the second hidden layer has 10 neurons. (If you didn’t write your code in the first place to accomodate different numbers of layers, after completing this part you may want to re-factor your code so that it can now handle different numbers of layers.) Using the data in classasgntrain1.dat as before, train and test the network and report on the probability of error. Comment on the difference in performance between the one-layer and the two-layer networks.  + Create a network with two hidden layers, with 300 neurons in the first (lowest) hidden layer and 100 neurons in the second hidden layer. Again, try different values of the step size parameter, plotting the MSE as a function of iteration. Use a minibatch of size 100. And again, test using the test data from the MNIST database.  + Train your network using momentum learning with β = 0.8, and repeat the above experiments.  + Turn in answers to questions, program listings, plots, comments, and observations.
         
        % Comment on how many iterations are needed, and how long (in clock time) it takes to train.
        % + Create a network with two hidden layers, where the first hidden layer has 5 neurons and the second hidden layer % has 10 neurons. (If you didn’t write your code in the first place to accomodate different numbers of layers, after % completing this part you may want to re-factor your code so that it can now handle different numbers of layers.) % Using the data in classasgntrain1.dat as before, train and test the network and report on the probability % of error. Comment on the difference in performance between the one-layer and the two-layer networks.
        % + Create a network with two hidden layers, with 300 neurons in the first (lowest) hidden layer and 100 neurons in % the second hidden layer. Again, try different values of the step size parameter, plotting the MSE as a function of % iteration. Use a minibatch of size 100. And again, test using the test data from the MNIST database.
        % + Train your network using momentum learning with β = 0.8, and repeat the above experiments.
        % + Turn in answers to questions, program listings, plots, comments, and observations.
    }

    \DeclareDocumentCommand{\appendix}{} {
        \section{Appendix}\label{app:code-listing:ten-class}
        \lstinputlisting[language=Python]{../python/deepnn.py}
    }


\begin{document}
    \main
\end{document}
